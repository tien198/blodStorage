import { list } from '@vercel/blob';
import fs from 'fs';
import path from 'path';
import { pipeline } from 'stream/promises';
import 'dotenv/config';
import { Stream } from 'stream';

// ƒê·∫£m b·∫£o b·∫°n ƒë√£ c√≥ bi·∫øn m√¥i tr∆∞·ªùng BLOB_READ_WRITE_TOKEN trong file .env

async function backupBlobStore() {
    console.log('üöÄ B·∫Øt ƒë·∫ßu qu√° tr√¨nh backup...');

    // 1. Li·ªát k√™ t·∫•t c·∫£ c√°c file trong Blob Store
    // L∆∞u √Ω: N·∫øu c√≥ qu√° nhi·ªÅu file, b·∫°n c·∫ßn d√πng pagination (cursor)
    const { blobs } = await list();

    const backupDir = './backup-data';
    if (!fs.existsSync(backupDir)) {
        fs.mkdirSync(backupDir);
    }

    console.log(`üì¶ T√¨m th·∫•y ${blobs.length} file.`);

    for (const blob of blobs) {
        const fileName = path.basename(blob.pathname);
        const dirname = path.dirname(blob.pathname);

        const destinationDir = path.join(backupDir, dirname);
        if (!fs.existsSync(destinationDir)) {
            fs.mkdirSync(destinationDir, { recursive: true });
        }

        const filePath = path.join(destinationDir, fileName);
        console.log(`‚¨áÔ∏è ƒêang t·∫£i: ${blob.pathname}`);

        // 2. T·∫£i file v·ªÅ
        const response = await fetch(blob.url);
        if (!response.ok) {
            console.error(`‚ùå L·ªói khi t·∫£i ${blob.pathname}: ${response.statusText}`);
            continue;
        }

        // 3. L∆∞u v√†o ·ªï c·ª©ng
        const fileStream = fs.createWriteStream(filePath);
        await pipeline(
            response.body as Stream.PipelineSource<any>,
            fileStream
        );
    }

    console.log('‚úÖ Sao l∆∞u ho√†n t·∫•t!');
}

backupBlobStore().catch(console.error);